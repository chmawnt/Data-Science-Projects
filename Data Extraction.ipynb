{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiHJoHTv18oqQ0/zn8wDiD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#importing required libraries\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import numpy as np\n","\n","import re\n","\n","#importing nltk library and stopwords\n","import nltk\n","import string\n","\n","\n","\n","#importing tokenize library\n","from nltk.tokenize import word_tokenize\n","\n","import nltk\n","nltk.download('punkt')\n","\n","\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#importing input file\n","df=pd.read_excel(r'C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\Input.xlsx')[['URL_ID','URL']]\n","\n","StopWords_Auditor=pd.read_csv(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_Auditor.txt\",header=None)\n","with open(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_Currencies.txt\") as f:\n","    data = [list(map(str, row.split())) for row in f.read().split('\\n\\n')]\n","\n","StopWords_Currencies = pd.DataFrame(data)\n","StopWords_Currencies = StopWords_Currencies.transpose()\n","StopWords_DatesandNumbers=pd.read_csv(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_DatesandNumbers.txt\",header=None)\n","StopWords_Generic=pd.read_csv(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_Generic.txt\",header=None)\n","StopWords_GenericLong=pd.read_csv(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_GenericLong.txt\",header=None)\n","StopWords_Geographic=pd.read_csv(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_Geographic.txt\",header=None)\n","StopWords_Names=pd.read_csv(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_Names.txt\",header=None)\n","\n","\n","\n","#importing master Dictionary\n","\n","positive=pd.read_csv(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\MasterDictionary\\positive-words.txt\",header=None)\n","with open(r\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\MasterDictionary\\negative-words.txt\") as f:\n","    data_neg = [list(map(str, row.split())) for row in f.read().split('\\n\\n')]\n","negative = pd.DataFrame(data_neg)\n","negative = negative.transpose()\n","\n","\n","\n","df=df.iloc[0:114]\n","print(df)\n","\n","orignal_df = df.copy()\n","\n","df.drop('URL_ID',axis=1,inplace=True)\n","\n","\n","\n","\n","\n","#################    Data Extraction     #######################\n","\n","#extracting text from all the url\n","url_id=1\n","df_data = pd.Series()\n","no_content_index = []\n","for i in range(0,len(df)):\n","\n","    j=df.iloc[i].values\n","    #print(i)\n","\n","    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}#giving user access\n","    page=requests.get(j[0],headers=headers)\n","    soup=BeautifulSoup(page.content,'html.parser')                #parsing url text\n","    content=soup.findAll(attrs={'class':'td-post-content'})                #extracting only text part\n","    if content:\n","      content=content[0].text.replace('\\xa0',\"  \").replace('\\n',\"  \")      #replace end line symbol with space\n","    else:\n","        no_content_index.append(i)\n","        print(f\"We find No Content at link {j} which is at index {i}\")\n","        continue\n","    title=soup.findAll(attrs={'class':'entry-title'})                           #extracting title of website\n","    title=title[16].text.replace('\\n',\"  \").replace('/',\"\")\n","    text=title+ '.' +content                                                    #merging title and content text\n","    text=np.array(text)                                                         #converting to array form\n","    text.reshape(1,-1)                                                          #changing shape to 1d\n","    df1 = pd.Series(text)                                                       #creating series data frame\n","   #  print(df1)\n","\n","\n","    df_data = df_data.append(df1).reset_index(drop=True)                        # getting all data\n","\n","    url_id+=1\n","\n","## droping no content/text link that found\n","print(no_content_index)                                                         #drop no content\n","orignal_df.drop(no_content_index, inplace = True)\n","print(f'no content {orignal_df}')\n","\n","print(\"extracted data\")\n","print(df_data)\n","\n","\n","                        ############     Data Analysis    ###########\n","\n","\n","# extracted files\n","\n","df_all = df_data.copy()\n","df_all = df_all.to_frame()\n","print(df_all.info())\n","\n","\n","output=pd.DataFrame()\n","#output.columns=['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n","\n","for row in df_all.iterrows():\n","    print(row)\n","    element = row[1]\n","    element = element.astype(str)\n","    a=element.str.split('([\\.]\\s)',expand=False)                               #splitting text on '.'\n","    b=a.explode()                                                              #converting to rows\n","    b=pd.DataFrame(b)                                                          #creating data frame\n","    b.columns=['abc']\n","    # print(b)\n","\n","    #removing . char from each rows\n","    def abcd(x):\n","        nopunc =[char for char in x if char != '.']\n","        return ''.join(nopunc)\n","    b['abc']=b['abc'].apply(abcd)\n","\n","    #print(b)\n","\n","    #replacing empty space with null values\n","\n","    c=b.replace('',np.nan,regex=True)\n","    c=c.mask(c==\" \")\n","    c=c.dropna()\n","    c.reset_index(drop=True,inplace=True)\n","    # print(c)\n","\n","    punc=[punc for punc in string.punctuation]\n","    print(punc)\n","\n","\n","    #creating func for removing stop words\n","\n","\n","    def text_process(text):\n","        nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','’','?']]\n","        nopunc=''.join(nopunc)\n","        txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])\n","        txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])\n","        txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])\n","        txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])\n","        txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])\n","        txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])\n","        return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])\n","\n","    #applying func for each row\n","    c['abc']=c['abc'].apply(text_process)\n","    print(c)\n","\n","\n","    print(f\"positive dataframe \\n {positive}\")\n","    print(f\"positive dataframe \\n {negative}\")\n","    positive.columns=['abc']\n","    negative.columns=['abc']\n","    positive['abc']=positive['abc'].astype(str)\n","    negative['abc']=negative['abc'].astype(str)\n","\n","    #positive and negative dictionary without stopwords\n","    positive['abc']=positive['abc'].apply(text_process)\n","    negative['abc']=negative['abc'].apply(text_process)\n","\n","\n","    #positive list\n","    length=positive.shape[0]\n","    post=[]\n","    for i in range(0,length):\n","        nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']\n","        nopunc=''.join(nopunc)\n","        post.append(nopunc)\n","\n","\n","\n","    #negative list\n","    length=negative.shape[0]\n","    neg=[]\n","    for i in range(0,length):\n","        nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']\n","        nopunc=''.join(nopunc)\n","        neg.append(nopunc)\n","\n","\n","    #tokenize\n","\n","    txt_list=[]\n","    length=c.shape[0]\n","    for i in range(0,length):\n","        txt=' '.join([word for word in c.iloc[i]])\n","        txt_list.append(txt)\n","\n","    #tokenization of text\n","    tokenize_text=[]\n","    for i in txt_list:\n","        tokenize_text+=(word_tokenize(i))\n","\n","    print(tokenize_text)\n","    print(len(tokenize_text))\n","\n","\n","\n","\n","    ### POSITIVE SCORE\n","\n","    positive_score=0\n","    for i in tokenize_text:\n","        if(i.lower() in post):\n","            positive_score+=1\n","    print('postive score=', positive_score)\n","\n","\n","    ### NEGATIVE SCORE\n","\n","    negative_score=0\n","    for i in tokenize_text:\n","        if(i.lower() in neg):\n","            negative_score+=1\n","    print('negative score=', negative_score)\n","\n","\n","    ### POLARITY SCORE\n","\n","    #Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n","    Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n","    print('polarity_score=', Polarity_Score)\n","\n","\n","    ### SUBJECTIVITY SCORE\n","\n","    #Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n","    subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)\n","    print('subjectivity_score',subjectiivity_score)\n","\n","\n","    ### AVG SENTENCE LENGTH\n","\n","    length=c.shape[0]\n","    avg_length=[]\n","    for i in range(0,length):\n","        avg_length.append(len(c['abc'].iloc[i]))\n","    avg_senetence_length=sum(avg_length)/len(avg_length)\n","    print('avg sentence length=', avg_senetence_length)\n","\n","\n","    ### PERCENTAGE OF COMPLEX WORDS\n","\n","    vowels=['a','e','i','o','u']\n","    count=0\n","    complex_Word_Count=0\n","    for i in tokenize_text:\n","        x=re.compile('[es|ed]$')\n","        if x.match(i.lower()):\n","            count+=0\n","        else:\n","            for j in i:\n","                if(j.lower() in vowels ):\n","                    count+=1\n","        if(count>2):\n","            complex_Word_Count+=1\n","        count=0\n","\n","    Percentage_of_Complex_words=complex_Word_Count/len(tokenize_text)\n","    print('percentag of complex words= ',Percentage_of_Complex_words)\n","\n","\n","    ### FOG INDEX\n","\n","    #Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","    Fog_Index = 0.4 * (avg_senetence_length + Percentage_of_Complex_words)\n","    print('fog index= ',Fog_Index )\n","\n","\n","\n","    ### AVG NUMBER OF WORDS PER SENTENCE\n","    length=c.shape[0]\n","    avg_length=[]\n","    for i in range(0,length):\n","          a=[word.split( ) for word in c.iloc[i]]\n","          avg_length.append(len(a[0]))\n","          a=0\n","    avg_no_of_words_per_sentence=sum(avg_length)/length\n","    print(\"avg no of words per sentence= \",avg_no_of_words_per_sentence)\n","\n","\n","\n","    ### COMPLEX WORD COUNT\n","\n","    vowels=['a','e','i','o','u']\n","    count=0\n","    complex_Word_Count=0\n","    for i in tokenize_text:\n","        x=re.compile('[es|ed]$')\n","        if x.match(i.lower()):\n","          count+=0\n","        else:\n","          for j in i:\n","            if(j.lower() in vowels ):\n","              count+=1\n","        if(count>2):\n","          complex_Word_Count+=1\n","        count=0\n","\n","    print('complex words count=',  complex_Word_Count)\n","\n","\n","\n","    ### WORD COUNT\n","\n","    word_count=len(tokenize_text)\n","    print('word count= ', word_count)\n","\n","\n","\n","    ###  SYLLABLE PER WORD\n","\n","    vowels=['a','e','i','o','u']\n","    count=0\n","    for i in tokenize_text:\n","        x=re.compile('[es|ed]$')\n","        if x.match(i.lower()):\n","          count+=0\n","        else:\n","          for j in i:\n","            if(j.lower() in vowels ):\n","              count+=1\n","    syllable_count=count\n","    print('syllable_per_word= ',syllable_count)\n","\n","\n","\n","    ### PERSONAL PRONOUNS\n","\n","    pronouns=['i','we','my','ours','us' ]\n","    count=0\n","    for i in tokenize_text:\n","        if i.lower() in pronouns:\n","          count+=1\n","    personal_pronouns=count\n","    print('personal pronouns= ',personal_pronouns )\n","\n","\n","    ### AVG WORD LENGTH\n","\n","    count=0\n","    for i in tokenize_text:\n","      for j in i:\n","        count+=1\n","    avg_word_length=count/len(tokenize_text)\n","    print('avg word= ', avg_word_length)\n","\n","\n","    data={'POSITIVE SCORE':positive_score,'NEGATIVE SCORE':negative_score,'POLARITY SCORE':Polarity_Score,'SUBJECTIVITY SCORE':subjectiivity_score,'AVG SENTENCE LENGTH':avg_senetence_length,'PERCENTAGE OF COMPLEX WORDS':Percentage_of_Complex_words,'FOG INDEX':Fog_Index,'AVG NUMBER OF WORDS PER SENTENCE':avg_no_of_words_per_sentence,'COMPLEX WORD COUNT':complex_Word_Count,'WORD COUNT':word_count,'SYLLABLE PER WORD':syllable_count,'PERSONAL PRONOUNS':personal_pronouns,'AVG WORD LENGTH':avg_word_length}\n","\n","    output=output.append(data,ignore_index=True)\n","\n","\n","## Reset index and merge for output structure\n","\n","orignal_df.reset_index(drop=True, inplace=True)\n","output.reset_index(drop=True, inplace=True)\n","df_final_output = pd.concat([orignal_df, output], axis=1)\n","print(df_final_output)\n","print(df_final_output.shape)\n","\n","df_final_output.to_csv('final_output.csv', index=False)\n","\n","\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"NFEGMBS2bjLF","executionInfo":{"status":"error","timestamp":1722542114041,"user_tz":240,"elapsed":182,"user":{"displayName":"nz asif","userId":"01164364696949326680"}},"outputId":"424493df-8d0f-433d-99f1-ca54f64c2405"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'C:/Users/nazia/Downloads/20211030 Test Assignment/Input.xlsx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-621cd044b249>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#importing input file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'C:/Users/nazia/Downloads/20211030 Test Assignment/Input.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'URL_ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mStopWords_Auditor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\nazia\\Downloads\\20211030 Test Assignment\\StopWords\\StopWords_Auditor.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1564\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1420\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m     ) as handle:\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/nazia/Downloads/20211030 Test Assignment/Input.xlsx'"]}]}]}